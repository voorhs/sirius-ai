{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –£–ª—É—á—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Ä–ø–ª–∞—Ç—ã —Å –ø–æ–º–æ—â—å—é NLP\n",
        "\n",
        "## –í–≤–µ–¥–µ–Ω–∏–µ\n",
        "\n",
        "**–ü–æ–º–Ω–∏—Ç–µ –∑–∞–¥–∞—á—É –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –¥–Ω—è?** –ú—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–ª–∏ –∑–∞—Ä–ø–ª–∞—Ç—É –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º —Å hh.ru, –Ω–æ —Ç–æ–≥–¥–∞ **–ø—Ä–∏—à–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** (–Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏, –æ–ø–∏—Å–∞–Ω–∏–µ), –ø–æ—Ç–æ–º—É —á—Ç–æ –º—ã –µ—â–µ –Ω–µ –∑–Ω–∞–ª–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–æ–º!\n",
        "\n",
        "### –ß—Ç–æ –±—ã–ª–æ –≤ –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å:\n",
        "\n",
        "- ‚ùå –£–¥–∞–ª–∏–ª–∏ `name_clean` (–Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏)\n",
        "- ‚ùå –£–¥–∞–ª–∏–ª–∏ `raw_description` (–æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏)\n",
        "- ‚ùå –£–¥–∞–ª–∏–ª–∏ `employer_name` (–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏)\n",
        "- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "- üìä –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: **Test MAE ‚âà 16,283 ‚ÇΩ**, **R¬≤ ‚âà 0.33**\n",
        "\n",
        "### –ß—Ç–æ –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å —Å–µ–≥–æ–¥–Ω—è:\n",
        "\n",
        "–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –∑–Ω–∞–µ–º NLP, –¥–∞–≤–∞–π—Ç–µ **–¥–æ–±–∞–≤–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞!\n",
        "\n",
        "–ú—ã –ø–æ–ø—Ä–æ–±—É–µ–º –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞:\n",
        "1. **CountVectorizer (Bag of Words)** ‚Äî –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤\n",
        "2. **Sentence Transformers** ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π\n",
        "\n",
        "–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏** (`name_clean`) ‚Äî —ç—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ä–æ—à–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç –ø–æ–∑–∏—Ü–∏—é.\n",
        "\n",
        "### –ü–ª–∞–Ω —Ä–∞–±–æ—Ç—ã:\n",
        "\n",
        "1. –ó–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä—ã\n",
        "2. –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–º baseline (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞) –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "3. –î–æ–±–∞–≤–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ CountVectorizer\n",
        "4. –î–æ–±–∞–≤–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ Sentence Transformers\n",
        "5. –°—Ä–∞–≤–Ω–∏–º –≤—Å–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —É–ª—É—á—à–µ–Ω–∏—è!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ –∏ –≤ –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1MQwhsmim8FUh0s8lwb63TEDWUMcH9xZT\n",
            "From (redirected): https://drive.google.com/uc?id=1MQwhsmim8FUh0s8lwb63TEDWUMcH9xZT&confirm=t&uuid=d81309ae-0c5a-4bad-b4a9-bc13aee59bb7\n",
            "To: /Users/voorhs/repos/sirius/sirius-ai/day_3/vacancy.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60.0M/60.0M [00:53<00:00, 1.12MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown --fuzzy \"https://drive.google.com/file/d/1MQwhsmim8FUh0s8lwb63TEDWUMcH9xZT/view?usp=sharing\"\n",
        "! unzip -q vacancy.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train —Ä–∞–∑–º–µ—Ä: (49051, 28)\n",
            "Test —Ä–∞–∑–º–µ—Ä: (12263, 28)\n",
            "\n",
            "–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:\n",
            "['id', 'name', 'employer_name', 'experience_name', 'schedule_name', 'key_skills_name', 'accept_handicapped', 'accept_kids', 'unified_address_city', 'unified_address_state', 'unified_address_region', 'unified_address_country', 'specializations_profarea_name', 'professional_roles_name', 'languages_name', 'raw_description', 'raw_branded_description', 'lemmaized_wo_stopwords_raw_description', 'lemmaized_wo_stopwords_raw_branded_description', 'if_foreign_language', 'is_branded_description', 'name_clean', 'employment_name', 'employer_id', 'employer_industries', 'salary_mean_net', 'lemmaized_wo_stopwords_raw_description_salaries', 'lemmaized_wo_stopwords_raw_branded_description_salaries']\n"
          ]
        }
      ],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (—É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ day_1/)\n",
        "train_df = pd.read_csv(\"train_split.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"test_split.tsv\", sep=\"\\t\")\n",
        "\n",
        "print(f\"Train —Ä–∞–∑–º–µ—Ä: {train_df.shape}\")\n",
        "print(f\"Test —Ä–∞–∑–º–µ—Ä: {test_df.shape}\")\n",
        "print(\"\\n–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n",
        "print(train_df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä—ã –Ω–∞–∑–≤–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π\n",
        "\n",
        "–≠—Ç–æ —Ç–æ—Ç —Å–∞–º—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –∫–æ—Ç–æ—Ä—ã–π –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–∑–≤–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π –∏ –∏—Ö –∑–∞—Ä–ø–ª–∞—Ç—ã\n",
        "print(\"–ü—Ä–∏–º–µ—Ä—ã –≤–∞–∫–∞–Ω—Å–∏–π:\\n\")\n",
        "examples = train_df[['name_clean', 'salary_mean_net']].dropna().sample(10, random_state=42)\n",
        "for idx, row in examples.iterrows():\n",
        "    print(f\"üíº {row['name_clean']}\")\n",
        "    print(f\"   üí∞ {row['salary_mean_net']:,.0f} ‚ÇΩ\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º –≤–∞–∫–∞–Ω—Å–∏–π\n",
        "print(\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—é name_clean:\\n\")\n",
        "print(f\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(train_df)}\")\n",
        "print(f\"–ó–∞–ø–æ–ª–Ω–µ–Ω–æ: {train_df['name_clean'].notna().sum()}\")\n",
        "print(f\"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {train_df['name_clean'].isna().sum()}\")\n",
        "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π: {train_df['name_clean'].nunique()}\")\n",
        "\n",
        "# –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è\n",
        "train_df['name_length'] = train_df['name_clean'].fillna('').str.len()\n",
        "print(f\"\\n–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è: {train_df['name_length'].mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "print(f\"–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {train_df['name_length'].median():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {train_df['name_length'].max():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline –º–æ–¥–µ–ª—å (–±–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
        "\n",
        "–°–Ω–∞—á–∞–ª–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –¥–Ω—è ‚Äî –æ–±—É—á–∏–º –º–æ–¥–µ–ª—å **–ë–ï–ó** —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
        "–≠—Ç–æ –Ω–∞—à baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∫ –≤ –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features_no_text(df, is_train=True):\n",
        "    \"\"\"\n",
        "    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ë–ï–ó —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π (–∫–∞–∫ –≤ –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # –£–¥–∞–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∫–∞–∫ –¥–µ–ª–∞–ª–∏ –≤ –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å)\n",
        "    text_cols = ['raw_description', 'raw_branded_description', \n",
        "                 'lemmaized_wo_stopwords_raw_description',\n",
        "                 'lemmaized_wo_stopwords_raw_branded_description',\n",
        "                 'lemmaized_wo_stopwords_raw_description_salaries',\n",
        "                 'lemmaized_wo_stopwords_raw_branded_description_salaries',\n",
        "                 'name_clean', 'employer_name', 'name']\n",
        "    \n",
        "    df = df.drop(columns=[col for col in text_cols if col in df.columns], errors='ignore')\n",
        "    \n",
        "    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "    cols_to_drop = ['id', 'salary_mean_net']\n",
        "    X = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
        "    \n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã–µ (–ø—Ä–æ—Å—Ç–æ–π label encoding)\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
        "    \n",
        "    for col in categorical_cols:\n",
        "        if X[col].dtype == 'object':\n",
        "            X[col] = X[col].astype('category').cat.codes\n",
        "        else:\n",
        "            X[col] = X[col].astype(int)\n",
        "    \n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π\n",
        "    X = X.fillna(X.median())\n",
        "    \n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è baseline\n",
        "X_train_baseline = prepare_features_no_text(train_df, is_train=True)\n",
        "y_train = train_df['salary_mean_net'].values\n",
        "\n",
        "X_test_baseline = prepare_features_no_text(test_df, is_train=False)\n",
        "y_test = test_df['salary_mean_net'].values\n",
        "\n",
        "print(f\"X_train_baseline shape: {X_train_baseline.shape}\")\n",
        "print(f\"X_test_baseline shape: {X_test_baseline.shape}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ë–ï–ó —Ç–µ–∫—Å—Ç–∞): {X_train_baseline.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –û–±—É—á–µ–Ω–∏–µ baseline –º–æ–¥–µ–ª–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º Ridge —Ä–µ–≥—Ä–µ—Å—Å–∏—é (–±—ã—Å—Ç—Ä–∞—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è)\n",
        "baseline_model = LassoCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100, 1000], random_state=RANDOM_STATE)\n",
        "baseline_model.fit(X_train_baseline, y_train)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred_baseline = baseline_model.predict(X_test_baseline)\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏\n",
        "baseline_mae = mean_absolute_error(y_test, y_pred_baseline)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
        "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BASELINE (–ë–ï–ó —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Test MAE:  {baseline_mae:,.2f} ‚ÇΩ\")\n",
        "print(f\"Test RMSE: {baseline_rmse:,.2f} ‚ÇΩ\")\n",
        "print(f\"Test R¬≤:   {baseline_r2:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "results = {\n",
        "    'Baseline (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞)': {\n",
        "        'MAE': baseline_mae,\n",
        "        'RMSE': baseline_rmse,\n",
        "        'R¬≤': baseline_r2,\n",
        "        'predictions': y_pred_baseline\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –ü–æ–¥—Ö–æ–¥ 1: CountVectorizer (Bag of Words)\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è: –ß—Ç–æ —Ç–∞–∫–æ–µ Bag of Words?\n",
        "\n",
        "**Bag of Words (–º–µ—à–æ–∫ —Å–ª–æ–≤)** ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–∞:\n",
        "\n",
        "1. –ë–µ—Ä–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Üí **—Å–ª–æ–≤–∞—Ä—å**\n",
        "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ\n",
        "3. –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä —á–∏—Å–µ–ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "```\n",
        "–î–æ–∫—É–º–µ–Ω—Ç 1: \"Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫\"\n",
        "–î–æ–∫—É–º–µ–Ω—Ç 2: \"Java —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫\"\n",
        "–î–æ–∫—É–º–µ–Ω—Ç 3: \"Python –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç\"\n",
        "\n",
        "–°–ª–æ–≤–∞—Ä—å: [python, java, —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç]\n",
        "\n",
        "–í–µ–∫—Ç–æ—Ä—ã:\n",
        "–î–æ–∫—É–º–µ–Ω—Ç 1: [1, 0, 1, 0]  # –µ—Å—Ç—å python, –µ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫\n",
        "–î–æ–∫—É–º–µ–Ω—Ç 2: [0, 1, 1, 0]  # –µ—Å—Ç—å java, –µ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫\n",
        "–î–æ–∫—É–º–µ–Ω—Ç 3: [1, 0, 0, 1]  # –µ—Å—Ç—å python, –µ—Å—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç\n",
        "```\n",
        "\n",
        "**CountVectorizer** –∏–∑ sklearn –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!\n",
        "\n",
        "### –í–∞–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "- `max_features` ‚Äî –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ (–±—Ä–∞—Ç—å —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ)\n",
        "- `min_df` ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞)\n",
        "- `max_df` ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞)\n",
        "- `ngram_range` ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, –Ω–æ –∏ –ø–∞—Ä—ã/—Ç—Ä–æ–π–∫–∏\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: –í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–∏–º–µ–Ω–∏—Ç—å CountVectorizer\n",
        "\n",
        "–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –º–µ—Å—Ç–∞ –≤ –∫–æ–¥–µ –Ω–∏–∂–µ:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö –≤–∞–∫–∞–Ω—Å–∏–π –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π\n",
        "train_texts = train_df['name_clean'].fillna('').values\n",
        "test_texts = test_df['name_clean'].fillna('').values\n",
        "\n",
        "print(\"–ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {i+1}. {train_texts[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ CountVectorizer —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "\n",
        "vectorizer = None  # TODO: —Å–æ–∑–¥–∞–π—Ç–µ CountVectorizer\n",
        "\n",
        "print(\"‚úì CountVectorizer —Å–æ–∑–¥–∞–Ω\" if vectorizer is not None else \"TODO: —Å–æ–∑–¥–∞–π—Ç–µ vectorizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ fit_transform –Ω–∞ train –∏ transform –Ω–∞ test\n",
        "\n",
        "X_train_text = None  # TODO: –≤–µ–∫—Ç–æ—Ä–∏–∑—É–π—Ç–µ train —Ç–µ–∫—Å—Ç—ã\n",
        "X_test_text = None   # TODO: –≤–µ–∫—Ç–æ—Ä–∏–∑—É–π—Ç–µ test —Ç–µ–∫—Å—Ç—ã\n",
        "\n",
        "print(f\"X_train_text shape: {X_train_text.shape if X_train_text is not None else 'TODO'}\")\n",
        "print(f\"X_test_text shape: {X_test_text.shape if X_test_text is not None else 'TODO'}\")\n",
        "if vectorizer is not None and hasattr(vectorizer, 'vocabulary_'):\n",
        "    print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(vectorizer.vocabulary_)}\")\n",
        "else:\n",
        "    print(\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: TODO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö –≤–∞–∫–∞–Ω—Å–∏–π\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤\n",
        "if vectorizer is not None and hasattr(vectorizer, 'vocabulary_') and X_train_text is not None:\n",
        "    word_counts = X_train_text.sum(axis=0).A1\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    top_words = sorted(zip(words, word_counts), key=lambda x: x[1], reverse=True)[:20]\n",
        "    \n",
        "    print(\"–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö –≤–∞–∫–∞–Ω—Å–∏–π:\\n\")\n",
        "    for word, count in top_words:\n",
        "        print(f\"  {word:20s} : {int(count):5d} —Ä–∞–∑\")\n",
        "else:\n",
        "    print(\"TODO: —Å–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ vectorizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4: –û–±—ä–µ–¥–∏–Ω–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —á–∏—Å–ª–æ–≤—ã–º–∏\n",
        "# –ü–æ–¥—Å–∫–∞–∑–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ hstack –∏–∑ scipy.sparse\n",
        "\n",
        "X_train_combined = None  # TODO: –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ X_train_baseline –∏ X_train_text\n",
        "X_test_combined = None   # TODO: –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ X_test_baseline –∏ X_test_text\n",
        "\n",
        "print(f\"X_train_combined shape: {X_train_combined.shape if X_train_combined is not None else 'TODO'}\")\n",
        "print(f\"X_test_combined shape: {X_test_combined.shape if X_test_combined is not None else 'TODO'}\")\n",
        "if X_train_text is not None:\n",
        "    print(f\"\\n–î–æ–±–∞–≤–ª–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_text.shape[1]}\")\n",
        "else:\n",
        "    print(\"\\n–î–æ–±–∞–≤–ª–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: TODO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5: –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "if X_train_combined is not None:\n",
        "    model_cv = LassoCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100, 1000], random_state=RANDOM_STATE)\n",
        "    model_cv.fit(X_train_combined, y_train)\n",
        "    \n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    y_pred_cv = model_cv.predict(X_test_combined)\n",
        "    \n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "    cv_mae = mean_absolute_error(y_test, y_pred_cv)\n",
        "    cv_rmse = np.sqrt(mean_squared_error(y_test, y_pred_cv))\n",
        "    cv_r2 = r2_score(y_test, y_pred_cv)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"–ú–û–î–ï–õ–¨ –° COUNTVECTORIZER\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Test MAE:  {cv_mae:,.2f} ‚ÇΩ\")\n",
        "    print(f\"Test RMSE: {cv_rmse:,.2f} ‚ÇΩ\")\n",
        "    print(f\"Test R¬≤:   {cv_r2:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å baseline\n",
        "    mae_improvement = baseline_mae - cv_mae\n",
        "    r2_improvement = cv_r2 - baseline_r2\n",
        "    \n",
        "    print(\"\\nüìà –£–ª—É—á—à–µ–Ω–∏–µ:\")\n",
        "    print(f\"   MAE: {mae_improvement:+,.2f} ‚ÇΩ ({mae_improvement/baseline_mae*100:+.2f}%)\")\n",
        "    print(f\"   R¬≤:  {r2_improvement:+.4f} ({r2_improvement/baseline_r2*100:+.2f}%)\")\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "    results['CountVectorizer'] = {\n",
        "        'MAE': cv_mae,\n",
        "        'RMSE': cv_rmse,\n",
        "        'R¬≤': cv_r2,\n",
        "        'predictions': y_pred_cv\n",
        "    }\n",
        "else:\n",
        "    print(\"TODO: —Å–Ω–∞—á–∞–ª–∞ –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –ü–æ–¥—Ö–æ–¥ 2: Sentence Transformers (–≠–º–±–µ–¥–¥–∏–Ω–≥–∏)\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è: –ß—Ç–æ —Ç–∞–∫–æ–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏?\n",
        "\n",
        "**–≠–º–±–µ–¥–¥–∏–Ω–≥–∏** ‚Äî —ç—Ç–æ –ø–ª–æ—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π.\n",
        "\n",
        "**–û—Ç–ª–∏—á–∏—è –æ—Ç Bag of Words:**\n",
        "\n",
        "| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Bag of Words | –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ |\n",
        "|----------------|--------------|------------|\n",
        "| –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ | –û–≥—Ä–æ–º–Ω—ã–π (—Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è) | –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π (–æ–±—ã—á–Ω–æ 384-768) |\n",
        "| –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å | –û—á–µ–Ω—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π | –ü–ª–æ—Ç–Ω—ã–π |\n",
        "| –°–µ–º–∞–Ω—Ç–∏–∫–∞ | –ù–µ—Ç (—Ç–æ–ª—å–∫–æ –ø–æ–¥—Å—á–µ—Ç) | –î–∞ (—É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–º—ã—Å–ª) |\n",
        "| –°–∏–Ω–æ–Ω–∏–º—ã | –†–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞ | –ü–æ—Ö–æ–∂–∏–µ –≤–µ–∫—Ç–æ—Ä—ã |\n",
        "| –ü–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ | –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è | –£—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è |\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "```\n",
        "\"Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫\" –∏ \"Python developer\" –±—É–¥—É—Ç:\n",
        "- –í BoW: —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ —Ä–∞–∑–Ω—ã–µ (—Ä–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞)\n",
        "- –í —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö: –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ (–ø–æ—Ö–æ–∂–∏–π —Å–º—ã—Å–ª)\n",
        "```\n",
        "\n",
        "**Sentence Transformers** ‚Äî —ç—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–µ–≥–∫–æ –ø–æ–ª—É—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: –í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–º–æ—â—å—é Sentence Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6: –ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "\n",
        "sentence_model = None  # TODO: –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å\n",
        "\n",
        "print(\"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\" if sentence_model is not None else \"TODO: –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 7: –ü–æ–ª—É—á–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π\n",
        "\n",
        "train_embeddings = None  # TODO: –ø–æ–ª—É—á–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è train\n",
        "test_embeddings = None   # TODO: –ø–æ–ª—É—á–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è test\n",
        "\n",
        "print(f\"train_embeddings shape: {train_embeddings.shape if train_embeddings is not None else 'TODO'}\")\n",
        "print(f\"test_embeddings shape: {test_embeddings.shape if test_embeddings is not None else 'TODO'}\")\n",
        "if train_embeddings is not None:\n",
        "    print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {train_embeddings.shape[1]}\")\n",
        "else:\n",
        "    print(\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: TODO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if train_embeddings is not None:\n",
        "    print(\"–ü—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –ø–µ—Ä–≤–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏:\")\n",
        "    print(f\"\\n–í–∞–∫–∞–Ω—Å–∏—è: {train_texts[0]}\")\n",
        "    print(\"\\n–≠–º–±–µ–¥–¥–∏–Ω–≥ (–ø–µ—Ä–≤—ã–µ 10 –∑–Ω–∞—á–µ–Ω–∏–π):\")\n",
        "    print(train_embeddings[0][:10])\n",
        "    print(f\"\\n...–≤—Å–µ–≥–æ {len(train_embeddings[0])} —á–∏—Å–µ–ª\")\n",
        "else:\n",
        "    print(\"TODO: —Å–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 8: –û–±—ä–µ–¥–∏–Ω–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "\n",
        "X_train_with_emb = None  # TODO: –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ X_train_baseline –∏ train_embeddings\n",
        "X_test_with_emb = None   # TODO: –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ X_test_baseline –∏ test_embeddings\n",
        "\n",
        "print(f\"X_train_with_emb shape: {X_train_with_emb.shape if X_train_with_emb is not None else 'TODO'}\")\n",
        "print(f\"X_test_with_emb shape: {X_test_with_emb.shape if X_test_with_emb is not None else 'TODO'}\")\n",
        "if train_embeddings is not None:\n",
        "    print(f\"\\n–î–æ–±–∞–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {train_embeddings.shape[1]}\")\n",
        "else:\n",
        "    print(\"\\n–î–æ–±–∞–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: TODO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 9: –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏\n",
        "if X_train_with_emb is not None:\n",
        "    model_emb = LassoCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100, 1000], random_state=RANDOM_STATE)\n",
        "    model_emb.fit(X_train_with_emb, y_train)\n",
        "    \n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    y_pred_emb = model_emb.predict(X_test_with_emb)\n",
        "    \n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "    emb_mae = mean_absolute_error(y_test, y_pred_emb)\n",
        "    emb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_emb))\n",
        "    emb_r2 = r2_score(y_test, y_pred_emb)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"–ú–û–î–ï–õ–¨ –° SENTENCE TRANSFORMERS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Test MAE:  {emb_mae:,.2f} ‚ÇΩ\")\n",
        "    print(f\"Test RMSE: {emb_rmse:,.2f} ‚ÇΩ\")\n",
        "    print(f\"Test R¬≤:   {emb_r2:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å baseline\n",
        "    mae_improvement = baseline_mae - emb_mae\n",
        "    r2_improvement = emb_r2 - baseline_r2\n",
        "    \n",
        "    print(\"\\nüìà –£–ª—É—á—à–µ–Ω–∏–µ:\")\n",
        "    print(f\"   MAE: {mae_improvement:+,.2f} ‚ÇΩ ({mae_improvement/baseline_mae*100:+.2f}%)\")\n",
        "    print(f\"   R¬≤:  {r2_improvement:+.4f} ({r2_improvement/baseline_r2*100:+.2f}%)\")\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "    results['Sentence Transformers'] = {\n",
        "        'MAE': emb_mae,\n",
        "        'RMSE': emb_rmse,\n",
        "        'R¬≤': emb_r2,\n",
        "        'predictions': y_pred_emb\n",
        "    }\n",
        "else:\n",
        "    print(\"TODO: —Å–Ω–∞—á–∞–ª–∞ –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤\n",
        "\n",
        "–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–∏—è!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –¢–∞–±–ª–∏—Ü–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "results_df = pd.DataFrame([\n",
        "    {'–ü–æ–¥—Ö–æ–¥': name, 'MAE (‚ÇΩ)': data['MAE'], 'RMSE (‚ÇΩ)': data['RMSE'], 'R¬≤': data['R¬≤']}\n",
        "    for name, data in results.items()\n",
        "])\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ MAE (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)\n",
        "results_df = results_df.sort_values('MAE (‚ÇΩ)')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´\")\n",
        "print(\"=\"*70)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ 1: MAE\n",
        "ax1 = axes[0]\n",
        "colors_list = ['#e74c3c' if i == len(results_df) - 1 else '#3498db' if i == len(results_df) - 2 else '#2ecc71' \n",
        "               for i in range(len(results_df))]\n",
        "bars1 = ax1.barh(results_df['–ü–æ–¥—Ö–æ–¥'], results_df['MAE (‚ÇΩ)'], color=colors_list)\n",
        "ax1.set_xlabel('MAE (‚ÇΩ)', fontsize=12)\n",
        "ax1.set_title('Mean Absolute Error\\n(–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)', fontsize=14, fontweight='bold')\n",
        "ax1.grid(alpha=0.3, axis='x')\n",
        "ax1.invert_yaxis()\n",
        "for i, bar in enumerate(bars1):\n",
        "    width = bar.get_width()\n",
        "    ax1.text(width, bar.get_y() + bar.get_height()/2, \n",
        "             f'{width:,.0f}', ha='left', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ 2: RMSE\n",
        "ax2 = axes[1]\n",
        "bars2 = ax2.barh(results_df['–ü–æ–¥—Ö–æ–¥'], results_df['RMSE (‚ÇΩ)'], color=colors_list)\n",
        "ax2.set_xlabel('RMSE (‚ÇΩ)', fontsize=12)\n",
        "ax2.set_title('Root Mean Squared Error\\n(–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)', fontsize=14, fontweight='bold')\n",
        "ax2.grid(alpha=0.3, axis='x')\n",
        "ax2.invert_yaxis()\n",
        "for i, bar in enumerate(bars2):\n",
        "    width = bar.get_width()\n",
        "    ax2.text(width, bar.get_y() + bar.get_height()/2, \n",
        "             f'{width:,.0f}', ha='left', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ 3: R¬≤\n",
        "ax3 = axes[2]\n",
        "bars3 = ax3.barh(results_df['–ü–æ–¥—Ö–æ–¥'], results_df['R¬≤'], color=colors_list)\n",
        "ax3.set_xlabel('R¬≤ Score', fontsize=12)\n",
        "ax3.set_title('R¬≤ Score\\n(–±–æ–ª—å—à–µ = –ª—É—á—à–µ)', fontsize=14, fontweight='bold')\n",
        "ax3.grid(alpha=0.3, axis='x')\n",
        "ax3.invert_yaxis()\n",
        "for i, bar in enumerate(bars3):\n",
        "    width = bar.get_width()\n",
        "    ax3.text(width, bar.get_y() + bar.get_height()/2, \n",
        "             f'{width:.4f}', ha='left', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –∑–∞—Ä–ø–ª–∞—Ç—ã', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 2: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))\n",
        "\n",
        "if len(results) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "for idx, (name, data) in enumerate(results.items()):\n",
        "    ax = axes[idx] if len(results) > 1 else axes[0]\n",
        "    \n",
        "    # Scatter plot\n",
        "    ax.scatter(y_test, data['predictions'], alpha=0.3, s=20, color=colors[idx % len(colors)])\n",
        "    \n",
        "    # –ò–¥–µ–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è\n",
        "    min_val = min(y_test.min(), data['predictions'].min())\n",
        "    max_val = max(y_test.max(), data['predictions'].max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='–ò–¥–µ–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')\n",
        "    \n",
        "    ax.set_xlabel('–†–µ–∞–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞ (‚ÇΩ)', fontsize=11)\n",
        "    ax.set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞ (‚ÇΩ)', fontsize=11)\n",
        "    ax.set_title(f'{name}\\nR¬≤ = {data[\"R¬≤\"]:.4f}', fontsize=12, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: —Ä–µ–∞–ª—å–Ω—ã–µ vs –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))\n",
        "\n",
        "if len(results) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "for idx, (name, data) in enumerate(results.items()):\n",
        "    ax = axes[idx] if len(results) > 1 else axes[0]\n",
        "    \n",
        "    errors = y_test - data['predictions']\n",
        "    \n",
        "    ax.hist(errors, bins=50, edgecolor='black', alpha=0.7, color=colors[idx % len(colors)])\n",
        "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='–ù–µ—Ç –æ—à–∏–±–∫–∏')\n",
        "    ax.axvline(errors.mean(), color='green', linestyle='--', linewidth=2, \n",
        "               label=f'–°—Ä–µ–¥–Ω–µ–µ: {errors.mean():,.0f} ‚ÇΩ')\n",
        "    \n",
        "    ax.set_xlabel('–û—à–∏–±–∫–∞ (‚ÇΩ)', fontsize=11)\n",
        "    ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=11)\n",
        "    ax.set_title(f'{name}\\n–°—Ç–¥. –æ—Ç–∫–ª. = {errors.std():,.0f} ‚ÇΩ', fontsize=12, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ê–Ω–∞–ª–∏–∑: –¥–ª—è –∫–∞–∫–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π —É–ª—É—á—à–∏–ª–∏—Å—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(results) >= 2:\n",
        "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º baseline —Å –ª—É—á—à–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–æ–¥—Ö–æ–¥–æ–º\n",
        "    baseline_errors = np.abs(y_test - results['Baseline (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞)']['predictions'])\n",
        "    \n",
        "    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥\n",
        "    text_approaches = [k for k in results.keys() if k != 'Baseline (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞)']\n",
        "    if text_approaches:\n",
        "        best_text = min(text_approaches, key=lambda k: results[k]['MAE'])\n",
        "        text_errors = np.abs(y_test - results[best_text]['predictions'])\n",
        "        \n",
        "        # –£–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏\n",
        "        improvements = baseline_errors - text_errors\n",
        "        \n",
        "        # –¢–æ–ø-10 –≤–∞–∫–∞–Ω—Å–∏–π —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —É–ª—É—á—à–µ–Ω–∏–µ–º\n",
        "        top_improved_idx = np.argsort(improvements)[-10:][::-1]\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"–¢–æ–ø-10 –≤–∞–∫–∞–Ω—Å–∏–π, –≥–¥–µ {best_text} –¥–∞–ª –Ω–∞–∏–±–æ–ª—å—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ:\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        for i, idx in enumerate(top_improved_idx, 1):\n",
        "            print(f\"\\n{i}. {test_df.iloc[idx]['name_clean']}\")\n",
        "            print(f\"   –†–µ–∞–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞: {y_test[idx]:,.0f} ‚ÇΩ\")\n",
        "            print(f\"   Baseline –æ—à–∏–±–∫–∞: {baseline_errors[idx]:,.0f} ‚ÇΩ\")\n",
        "            print(f\"   {best_text} –æ—à–∏–±–∫–∞: {text_errors[idx]:,.0f} ‚ÇΩ\")\n",
        "            print(f\"   ‚úì –£–ª—É—á—à–µ–Ω–∏–µ: {improvements[idx]:,.0f} ‚ÇΩ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –í—ã–≤–æ–¥—ã\n",
        "\n",
        "### –ß—Ç–æ –º—ã —É–∑–Ω–∞–ª–∏:\n",
        "\n",
        "1. **–¢–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–∂–Ω—ã!** –ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ\n",
        "\n",
        "2. **CountVectorizer (Bag of Words)**:\n",
        "   - ‚úÖ –ü—Ä–æ—Å—Ç–æ–π –∏ –±—ã—Å—Ç—Ä—ã–π\n",
        "   - ‚úÖ –õ–µ–≥–∫–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å (–≤–∏–¥–∏–º –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞)\n",
        "   - ‚ùå –ù–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–µ–º–∞–Ω—Ç–∏–∫—É\n",
        "   - ‚ùå –ë–æ–ª—å—à–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã) => –¥–æ–ª—å—à–µ —É—á–∏—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
        "\n",
        "3. **Sentence Transformers (–≠–º–±–µ–¥–¥–∏–Ω–≥–∏)**:\n",
        "   - ‚úÖ –ü–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª —Ç–µ–∫—Å—Ç–∞\n",
        "   - ‚úÖ –ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (–º–µ–Ω—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) => –±—ã—Å—Ç—Ä–µ–µ —É—á–∏—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
        "   - ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏\n",
        "   - ‚ùå –°–ª–æ–∂–Ω–µ–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å\n",
        "\n",
        "4. **–î–ª—è –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏** (–∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π) –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞ –¥–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å baseline!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
