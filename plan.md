это был наш изначальный план, в некоторых местах мы от него довольно сильно отходили

- Лекция 1: Основы глубинного обучения (с инженерной точки зрения как минимизация некоторой функции потерь). Полносвязаные слои. Обратное распространение ошибки. Тензоры, матричное умножение.
- Семинар 1.1: Основы PyTorch: тензоры, backprop. Слои pytorch
- Семинар 1.2: Цикл обучения. Примеры с torch-циклами для решения разных задач DL (табличные, изображения, регрессия, классификация).
- Домашка:
    - простейший цикл обучения в pytorch
    - операции с тензорами: создание, перенесение

- Лекция 2: Задачи компьютерного зрения: классификация, детекция, сегментация, поиск. Предобработка: дилатация и проч аугментации. Свёртки и свёрточные сети: VGG, Resnet, Unet.
- Семинар 1.1: Дообучение свёрточных сетей (пример с ResNet, у ноутбука четкий посыл)
- Семинар 1.2 Использование моделей из HuggingFace для задач CV.
- Домашка: повторить действия из семинара 1.2 на другом датасете и на другой задаче

- Лекция 3:
    - Основы NLP (токенизация, BOW)
    - Эмбединги слов (w2v)
    - Кратко о рекуррентных сетях (добавление зависимостей между словами + обновление репрезентаций)
    - Внимание в RNN (более крутое добавление зависимостей между словами)
    - Самовнимание
    - Предобученные трансформеры (BERT, LLM). Применение к CV. Мультимодальные модели (CLIP).
- Семинар 3.1(саша): ASR с помощью Whisper (потому что тоже трансформер + полезно). Git: version-control, commit, branch.
- Семинар 3.2: word2vec (gensim), hf transformers: BERT, GPT. Промт инжиниринг, LLM HTTP API.
- Домашка: общение с LLM в аудио формате ASR + TTS (тоже заполнение ноутбука, надо сделать)

- Лекция 4: Диффузионные модели. Stable Diffusion. ControlNet. Перенос стиля. (материалы есть)
- Семинар 1: pandas, matplotlib, seaborn, plotly
- Семинар 2: streamlit, aiogram